{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{align}\n",
       "\\text{Линейная регрессия, приближающая некоторую зависимость:}\\\\\n",
       "\\\\\n",
       "\\hat{y_i} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... + \\beta_n x_{n,i}   (1)\\\\\n",
       "\\\\\n",
       "\\text{Простейший вариант:} \\\\\\\\\n",
       "\\hat{y_i} = \\beta_0 + \\beta_1 x_{i}\\\\\\\\\n",
       "\\text{Математически коэффициенты } \\beta_0, \\beta_1, \\beta_2 ... \\beta_n \\text{ могут быть оценены с помощью формулы:} \\\\\n",
       "\\\\\n",
       "\\hat{\\beta} = (X^{T}X)^{-1} X^{T}Y \\\\\\\\\n",
       "\\text{Где X - матрица признаков, Y - известная зависимая величина, которую мы хотим приблизить с помощью модели.}\\\\\n",
       "\\text{Чтобы вычислить коэффициенты модели, необходимо ввести функцию ошибки, которую необходимо минимизировать}\\\\\n",
       "\\text{Обозначим ei как ошибку модели на точке с номером i, таким образом:}\\\\\\\\\n",
       "e_i = y_i - \\hat{y_i}\\\\\\\\\n",
       "\\text{Где y^  моделируемый результат, y известный результат:}\\\\\n",
       "\\text{Объявим функцию ошибки:}\\\\\\\\\n",
       "\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\\\\\\\\\n",
       "\\text{Модель регрессии мы можем перезаписать в матричной форме}\\\\\\\\\n",
       "\\hat{Y} = X\\beta \\\\\\\\\n",
       "\\text{Мы знаем, что:}\\\\\\\\\n",
       "\\sum_{i=1}^n e_i^2 = E^{T}E \\\\\\\\\n",
       "\\text{Мы хотим минимизировать общую квадратичную ошибку, }\\\\\n",
       "\\text{так, чтобы значение приведенного выражения, было маленьким, насколько это возможно}\\\\\\\\\n",
       "E^{T}E = (Y-\\hat{Y})^{T} (Y-\\hat{Y}) \\\\\\\\\n",
       "\\text{Это выражение эквивалентно следующему: }\\\\\\\\\n",
       "E^{T}E = (Y-X\\beta)^{T} (Y-X\\beta) \\\\\\\\\n",
       "\\text{Мы хотим найти значения β, при которых значения данного выражения минимально}\\\\\n",
       "\\text{Для этого продиффиренцируем его по β и приравняем 0. Используем \"chain rule\"}\\\\\n",
       "\\text{}\\\\\n",
       "\\frac{dE^{T}E}{d\\beta} = - 2 X^{T}Y + 2 X^{T}X\\beta = 0\\\\\\\\\n",
       "\\text{Упрощаем:}\\\\\\\\\n",
       "X^{T}X\\beta = X^{T}Y\\\\\\\\\n",
       "\\text{В итоге, коэффициенты можно найти по формуле:}\\\\\\\\\n",
       "\\beta = (X^{T}X)^{-1} X^{T}Y\n",
       "\\\\\\\\\n",
       "\\text{Мы нашли требуемое решение, однако, с точки зрения производительности, проблемой является}\\\\\n",
       "\\text{вычисление обратной матрицы, которое сложно произвести, если датасет большой.}\\\\\n",
       "\\text{В этой ситуации оптимальным является к примеру стохастический градиентный спуск.}\\\\\\\\\n",
       "\\text{Дополнение: нахождение обратной матрицы:}\\\\\n",
       "A^{-1}=\\frac{1}{|A|}A_+^{T}\\\\\\\\\n",
       "\\text{Где } |A| \\text{ - определитель матрицы } A \\text{, } A_+^{T} \\text{ - транспонированная матрица алгебраических дополнений}\n",
       "\\end{align}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Линейная регрессия, приближающая некоторую зависимость:}\\\\\n",
    "\\\\\n",
    "\\hat{y_i} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... + \\beta_n x_{n,i}   (1)\\\\\n",
    "\\\\\n",
    "\\text{Простейший вариант:} \\\\\\\\\n",
    "\\hat{y_i} = \\beta_0 + \\beta_1 x_{i}\\\\\\\\\n",
    "\\text{Математически коэффициенты } \\beta_0, \\beta_1, \\beta_2 ... \\beta_n \\text{ могут быть оценены с помощью формулы:} \\\\\n",
    "\\\\\n",
    "\\hat{\\beta} = (X^{T}X)^{-1} X^{T}Y \\\\\\\\\n",
    "\\text{Где X - матрица признаков, Y - известная зависимая величина, которую мы хотим приблизить с помощью модели.}\\\\\n",
    "\\text{Чтобы вычислить коэффициенты модели, необходимо ввести функцию ошибки, которую необходимо минимизировать}\\\\\n",
    "\\text{Обозначим ei как ошибку модели на точке с номером i, таким образом:}\\\\\\\\\n",
    "e_i = y_i - \\hat{y_i}\\\\\\\\\n",
    "\\text{Где y^  моделируемый результат, y известный результат:}\\\\\n",
    "\\text{Объявим функцию ошибки:}\\\\\\\\\n",
    "\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y_i})^2\\\\\\\\\n",
    "\\text{Модель регрессии (1) мы можем перезаписать в матричной форме}\\\\\\\\\n",
    "\\hat{Y} = X\\beta \\\\\\\\\n",
    "\\text{Мы знаем, что:}\\\\\\\\\n",
    "\\sum_{i=1}^n e_i^2 = E^{T}E \\\\\\\\\n",
    "\\text{Мы хотим минимизировать общую квадратичную ошибку, }\\\\\n",
    "\\text{так, чтобы значение приведенного выражения, было маленьким, насколько это возможно}\\\\\\\\\n",
    "E^{T}E = (Y-\\hat{Y})^{T} (Y-\\hat{Y}) \\\\\\\\\n",
    "\\text{Это выражение эквивалентно следующему: }\\\\\\\\\n",
    "E^{T}E = (Y-X\\beta)^{T} (Y-X\\beta) \\\\\\\\\n",
    "\\text{Мы хотим найти значения β, при которых значения данного выражения минимально}\\\\\n",
    "\\text{Для этого продиффиренцируем его по β и приравняем 0. Используем \"chain rule\"}\\\\\n",
    "\\text{}\\\\\n",
    "\\frac{dE^{T}E}{d\\beta} = - 2 X^{T}Y + 2 X^{T}X\\beta = 0\\\\\\\\\n",
    "\\text{Упрощаем:}\\\\\\\\\n",
    "X^{T}X\\beta = X^{T}Y\\\\\\\\\n",
    "\\text{В итоге, коэффициенты можно найти по формуле:}\\\\\\\\\n",
    "\\beta = (X^{T}X)^{-1} X^{T}Y\n",
    "\\\\\\\\\n",
    "\\text{Мы нашли требуемое решение, однако, с точки зрения производительности, проблемой является}\\\\\n",
    "\\text{вычисление обратной матрицы, которое сложно произвести, если датасет большой.}\\\\\n",
    "\\text{В этой ситуации оптимальным является к примеру стохастический градиентный спуск.}\\\\\\\\\n",
    "\\text{Дополнение: нахождение обратной матрицы:}\\\\\n",
    "A^{-1}=\\frac{1}{|A|}A_+^{T}\\\\\\\\\n",
    "\\text{Где } |A| \\text{ - определитель матрицы } A \\text{, } A_+^{T} \\text{ - транспонированная матрица алгебраических дополнений}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where X is the original input data and Y\n",
    "\n",
    "is the variable that we want to estimate. This follows from minimizing the error. I will proove this before making a small practical point.\n",
    "\n",
    "Let ei\n",
    "be the error the linear regression makes at point i. Then:\n",
    "    \n",
    "We want to minimize the total square error, such that the following expression should be as small as possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
